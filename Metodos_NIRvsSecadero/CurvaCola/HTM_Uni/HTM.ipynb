{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos librerías y definimos un par de funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from nupic.data.file_record_stream import FileRecordStream\n",
    "from htm_anomaly_detection import HTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return len(f.readlines())\n",
    "\n",
    "def getRunningTime(filepath, indicator):\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        lines = csvfile.readlines()\n",
    "        targetLine = lines[indicator]\n",
    "        lastime = targetLine.split(',')[0]\n",
    "        return datetime.datetime.strptime(lastime, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establecemos los parámetros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parameters that we need to define：\n",
    "    _TIMEOFDAY: internal buffer, see this: https://nupic.docs.numenta.org/1.0.3/api/algorithms/encoders.html\n",
    "    _STREAM_BUFFER: The data stream buffer, which decides how many rows of data that should be writed into csv cache file each time\n",
    "    _USE_SAVED_MODEL: Use saved model or not\n",
    "'''\n",
    "_TIMEOFDAY = (21,4)\n",
    "_STREAM_BUFFER = 60\n",
    "_USE_SAVED_MODEL = False\n",
    "\n",
    "\n",
    "'''\n",
    "Initialize HTM anomaly detection object\n",
    "'''\n",
    "htm = HTM(use_saved_model = _USE_SAVED_MODEL)\n",
    "\n",
    "\n",
    "'''\n",
    "Run the code below once to set encoders, SP, and TM parameters.\n",
    "You should define these parameters by yourself:\n",
    "    - minval (in scalarEncoderArgs): The minimum possible value for this input\n",
    "    - maxval (in scalarEncoderArgs): The maximum possible value for this input\n",
    "    - name (in scalarEncoderArgs): The name of this input\n",
    "    - clipInput (in scalarEncoderArgs): Clip input if the value exceed the min/max values. \n",
    "After this, these parameters will be writed into seperate files.\n",
    "\n",
    "* If you don't want to change these parameters anymore, then you should just read them from files \n",
    "and comment out the codes below in this cell.\n",
    "'''\n",
    "encoder_params = {\n",
    "   'NIRvsSecadero':{\n",
    "     \"minval\": -50,\n",
    "     \"maxval\": 50,\n",
    "     \"w\": 21,\n",
    "     \"periodic\": False,\n",
    "     \"n\": 50,\n",
    "     \"radius\": 0,\n",
    "     \"resolution\": 0,\n",
    "     \"name\": \"NIRvsSecadero\",\n",
    "     \"verbosity\": 0,\n",
    "     \"clipInput\": False,\n",
    "     \"forced\": False,\n",
    "   },\n",
    "\n",
    "   'fecha':{\n",
    "     \"season\": 0,\n",
    "     \"dayOfWeek\": 0,\n",
    "     \"weekend\": 0,\n",
    "     \"holiday\": 0,\n",
    "     \"timeOfDay\": _TIMEOFDAY,\n",
    "     \"customDays\": 0,\n",
    "     \"name\": \"fecha\",\n",
    "     \"forced\": False\n",
    "   }\n",
    "}\n",
    "\n",
    "# save encoder parameters\n",
    "if not os.path.exists('./temp'):\n",
    "    os.makedirs('./temp')\n",
    "    \n",
    "htm.setEncoderParams('./temp/encoders.json', encoder_params)\n",
    "\n",
    "_SP_PARAMS = {\n",
    "   'SP':{\n",
    "       \"spatialImp\": \"cpp\",\n",
    "       \"globalInhibition\": 1,\n",
    "       \"columnCount\": 2048,\n",
    "       \"inputWidth\": 0,\n",
    "       \"numActiveColumnsPerInhArea\": 40,\n",
    "       \"seed\": 1956,\n",
    "       \"potentialPct\": 0.8,\n",
    "       \"synPermConnected\": 0.1,\n",
    "       \"synPermActiveInc\": 0.0001,\n",
    "       \"synPermInactiveDec\": 0.0005,\n",
    "       \"boostStrength\": 0.0,\n",
    "   }\n",
    "}\n",
    "\n",
    "_TM_PARAMS = {\n",
    "   'TM':{\n",
    "       \"columnCount\": 2048,\n",
    "       \"cellsPerColumn\": 32,\n",
    "       \"inputWidth\": 2048,\n",
    "       \"seed\": 1960,\n",
    "       \"temporalImp\": \"cpp\",\n",
    "       \"newSynapseCount\": 20,\n",
    "       \"maxSynapsesPerSegment\": 32,\n",
    "       \"maxSegmentsPerCell\": 128,\n",
    "       \"initialPerm\": 0.21,\n",
    "       \"permanenceInc\": 0.1,\n",
    "       \"permanenceDec\": 0.1,\n",
    "       \"globalDecay\": 0.0,\n",
    "       \"maxAge\": 0,\n",
    "       \"minThreshold\": 9,\n",
    "       \"activationThreshold\": 12,\n",
    "       \"outputType\": \"normal\",\n",
    "       \"pamLength\": 3,\n",
    "   }\n",
    "}\n",
    "\n",
    "# save SP and TM parameters\n",
    "htm.setEncoderParams('./temp/SP.json', _SP_PARAMS)\n",
    "htm.setEncoderParams('./temp/TM.json', _TM_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora los cargamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load parameters from json files and build data stream reader to read data from cache.\n",
    "* Maybe we just hard-code these path? There are many of them for users to define.\n",
    "'''\n",
    "\n",
    "NIRvsSecadero = htm.getEncoderParams('./temp/encoders.json', 'NIRvsSecadero')\n",
    "\n",
    "fecha = htm.getEncoderParams('./temp/encoders.json', 'fecha')\n",
    "SPArgs = htm.getEncoderParams('./temp/SP.json', 'SP')\n",
    "TMArgs = htm.getEncoderParams('./temp/TM.json', 'TM')\n",
    "\n",
    "input01_recordParams = {\n",
    "  \"NIRvsSecadero\": NIRvsSecadero,\n",
    "  \"fecha\": fecha,\n",
    "}\n",
    "\n",
    "# define the data souce\n",
    "streamReader1 = FileRecordStream(streamID = 'Datos/Entrada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamos las filas, que serán las iteracciones del bucle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20003"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filas = count_rows('Datos/Entrada.csv')\n",
    "filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network01 = htm.createNetwork(datasource=streamReader1, recordParams=input01_recordParams, spatialParams=SPArgs, temporalParams=TMArgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.99 %  % \n",
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Looping network.run() to get iterative prediction from data cache.\n",
    "\n",
    "* network.run(1) means run this network once on next row of data in csv cache.\n",
    "\n",
    "Before you run this cell, I already called data_simulator.getBatchData2csv once in the \n",
    "above cell to initialize the first batch of data. After this, the new data will be required \n",
    "by the while loop below when it reach the end of cache.\n",
    "'''\n",
    "iteration = 0\n",
    "anomalias = []\n",
    "def run_network_once():\n",
    "        fed_in_data01, anomalyLikelihood1 = htm.run(network01)\n",
    "        anomalias.append(anomalyLikelihood1)\n",
    "        runTime = getRunningTime('Datos/Entrada.csv', (iteration % _STREAM_BUFFER) + 3)\n",
    "        #print 'Running time:', runTime, 'fed_in_data01:', fed_in_data01,' anomaly likelihood:', anomalyLikelihood1, iteration\n",
    "\n",
    "while(iteration+3 < filas):\n",
    "    run_network_once()\n",
    "    iteration += 1\n",
    "    if iteration % 10 == 0: \n",
    "        print round(float(iteration)/float(filas)*100,2),\"% \\r\",\n",
    "'''\n",
    "Save the model when breaking from while loop.\n",
    "\n",
    "You should define your button in SENSEI to break this while loop and save the model \n",
    "because we usually set looping as while(1) instead of while(iteration < n)\n",
    "'''\n",
    "if not os.path.exists('./models'):\n",
    "    os.makedirs('./models')\n",
    "    \n",
    "htm.save_network(network01, './models/network1.nta')\n",
    "\n",
    "print '\\nComplete.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Datos/anomalias.csv\",anomalias,delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e325c22bd2b0f7fc7f089eb4ebd33121f88937d04f3cf585400b3ad2484dfe0"
  },
  "kernelspec": {
   "display_name": "Python 2.7.16 ('.env27': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
